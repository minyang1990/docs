# 令人失望，deepseek-R1.1的学术文本处理能力远不如【长上下文负载下】的gemini-2.5pro - 开发调优 - LINUX DO
[令人失望，deepseek-R1.1的学术文本处理能力远不如【长上下文负载下】的gemini-2.5pro - 开发调优 - LINUX DO](https://linux.do/t/topic/689522/2) 

  令人失望，deepseek-R1.1的学术文本处理能力远不如【长上下文负载下】的gemini-2.5pro - 开发调优 - LINUX DO                                              

[跳到主要内容](#main-container)

 [![](https://linux.do/uploads/default/original/3X/9/d/9dd49731091ce8656e94433a26a3ef36062b3994.png)](/) 

[令人失望，deepseek-R1.1的学术文本处理能力远不如【长上下文负载下】的gemini-2.5pro](/t/topic/689522)


==========================================================================

[开发调优](/c/develop/4)

[人工智能](/tag/人工智能)

*   ​

*    ![](https://linux.do/letter_avatar/niyan2025/96/5_d44a9b381edc88181525e3c8350177ca.png)
       [ 4 ](# "4 个未读通知") 

*   [话题](/latest "所有话题")
*   [我的帖子](/u/niyan2025/activity/drafts "我的未发布草稿")
*   [关于](/about "关于此网站的更多详细信息")
*   [即将到来的活动](/upcoming-events "即将到来的活动")
*   更多

外部链接

*   [Status](https://status.linux.do)
*   [T-Shirt](https://shanwaiyoushan.taobao.com)
*   [Connect](https://connect.linux.do)
*   [Webmail](https://webmail.linux.do)
*   [Channel](https://t.me/linux_do_channel)
*   [Telegram](https://t.me/ja_netfilter_group)

类别

*   [开发调优](/c/develop/4 "此版块包含开发、测试、调试、部署、优化、安全等方面的内容")
*   [资源荟萃](/c/resource/14 "包括软件分享、开源仓库、视频课程、书籍等分享")
*   [文档共建](/c/wiki/42 "佬友化身翰林学士，一起来编书了。")
*   [跳蚤市场](/c/trade/10 "交易相关的版块，包含实体和虚拟物品供求、拼车等等")
*   [非我莫属](/c/job/27 "学成文武艺，货与帝王家。招聘/求职分类，只能发此类信息。")
*   [读书成诗](/c/reading/32 "跟着佬友们一起在论坛读完一本书是什么体验？")
*   [扬帆起航](/c/startup/46 "扬帆起航，目标是星辰大海！")
*   [前沿快讯](/c/news/34 "前沿快讯，不出门能知天下事。")
*   [网络记忆](/c/feeds/92 "网络是有记忆的，确信！")
*   [福利羊毛](/c/welfare/36 "正经人谁花那个钱啊～ 此版块供羊毛、抽奖等福利使用。")
*   [搞七捻三](/c/gossip/11 "闲聊吹水的板块。不得讨论政治、色情等违规内容。")
*   [运营反馈](/c/feedback/2 "有关此站点、其组织、运作方式以及如何改进的讨论。")
*   [深海幽域](/c/muted/45 "冰山下的深海。帖子不会上信息流、不会被论坛搜索。")
*   [所有类别](/categories)

标签

*   [人工智能](/tag/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD)
*   [公告](/tag/%E5%85%AC%E5%91%8A)
*   [快问快答](/tag/%E5%BF%AB%E9%97%AE%E5%BF%AB%E7%AD%94)
*   [抽奖](/tag/%E6%8A%BD%E5%A5%96)
*   [精华神帖](/tag/%E7%B2%BE%E5%8D%8E%E7%A5%9E%E5%B8%96)
*   [所有标签](/tags)

消息

*   [收件箱](/u/niyan2025/messages)

*   [我的消息串](/chat/threads "我的消息串")

频道

*   [常规频道](/chat/c/general/2 "🈲 禁止在聊天频道里发送 打卡 等无意义信息，被举报会喜提 禁言1小时 。")

直接消息

*    [![](https://linux.do/user_avatar/linux.do/xronus/48/130867_2.png)  Anivix](/chat/c/anivix/32458 "与 Anivix 聊天") 
*    [![](https://linux.do/user_avatar/linux.do/huan/48/293194_2.png)  焕昭君](/chat/c/%E7%84%95%E6%98%AD%E5%90%9B/43057 "与 焕昭君 聊天") 
*    [![](https://linux.do/user_avatar/linux.do/wenjuhe/48/672301_2.png)  奇妙进化小河马🦛   ![](https://linux.do/images/emoji/twemoji/hippopotamus.png?v=14)](/chat/c/%E5%A5%87%E5%A6%99%E8%BF%9B%E5%8C%96%E5%B0%8F%E6%B2%B3%E9%A9%AC%F0%9F%A6%9B/45968 "与 奇妙进化小河马🦛 聊天") 

聊天

Default

​ ​ ​

**真诚**、**友善**、**团结**、**专业**，共建你我引以为荣之社区。[《常见问题解答》](/faq)

[

❤️ 再忍不住，也不要做这种事啊 ❤️

](https://linux.do/t/topic/482293)

[令人失望，deepseek-R1.1的学术文本处理能力远不如【长上下文负载下】的gemini-2.5pro](/t/topic/689522)


==========================================================================

[开发调优](/c/develop/4)

[人工智能](/tag/人工智能)

您已选择 **0** 个帖子。

全选

取消选择

​

[5月 30 日](/t/topic/689522/1 "跳到第一个帖子")

2 / 42

5月 30 日

[2 小时](/t/topic/689522/42)

​

[![](https://linux.do/user_avatar/linux.do/user1164/96/426070_2.png)
](/u/user1164)

[欣 郁](/u/user1164)

[user1164](/u/user1164)

1

[3 小时](/t/topic/689522?u=niyan2025 "发布日期")

我自己开发了一个MCP学术工具，用以撰写医学类综述。其逻辑具体是

1.  在pubmed检索关键词后，获得文献信息，并下载原文pdf（若无OA或sci-hub，用摘要替代原文）
2.  将原文pdf转化为markdown，并分割为文本块，写入本地向量数据库
3.  基于文献摘要构建综述框架
4.  基于综述框架，使用RAG的方式检索文本块，用来填充综述内容

举例，我的综述框架如图：  

[![](https://linux.do/uploads/default/optimized/4X/d/2/5/d253da819323ca1dc5d69330a099fbe9548a91dc_2_289x500.png)

综述框架1969×3397 1.23 MB

](https://linux.do/uploads/default/original/4X/d/2/5/d253da819323ca1dc5d69330a099fbe9548a91dc.png "综述框架")

执行第四步时，我有两个策略：

**策略一（gemini一气呵成）**：用LLM以综述框架的一级标题为分割，逐个检索综述框架片段，形成综述片段，最后再拼成完整的综述。  
策略一中，综述框架片段检索后返还的文本块的token大概6k-7k，综述片段大概2k-4k，相当于每个综述片段的撰写都需要6k-7k的input和2k-3k的output，并且所有综述片段的撰写都在同一个聊天窗口，这对LLM的上下文负载压力极大（例如在写第六个综述片段时，上下文可能已经60k了），只有gemini2.5 pro才能完成。

**策略二（deepseek分块）**：以综述框架的一级标题为分割，用LLM逐个检索综述框架片段，最后形成综述片段。不同的是我开发了一个新MCP，将“LLM根据综述框架检索文本块后撰写综述片段”这个事情单独在一个新的聊天窗口处理，然后只返还写好的综述片段给原聊天窗口。  
这样，“每个综述框架片段扩写成综述片段”的事情就是独立的，都只有大概6k-7k的input和2k-3k的output，这样我就可以让普通的LLM来处理，比如最近更新的deepseek-R1.1。

————————  
然而事与愿违。我用一样的综述框架，比较不同策略的结果，发现“gemini一气呵成版”把“deepseek分块版”完爆了。大家可以自行比较（文末）。

这令我非常失望。因为gemini2.5 pro是在高上下文负载的情况下，表现居然还比deepseek-R1.1好。

更深层次的是，我是想把这个MCP推广给身边的朋友，但普通人多半没有渠道获得gemini2.5——如果想要能处理长文本且不截断的gemini2.5pro，只能用vertex版（常见的AI studio的gemini2.5pro api也会频繁截断），给使用造成了极大的门槛（需要gcp付费账户和可部署new\_api的主机）。一般国人能用到的最好的LLM也就是deepseek了。

只能说国产LLM任重道远。。。。

另外策略二中，改用gpt-4.1等其他128k上下文的LLM也可以，但使用成本会很高。

附：大家可以看一下两种策略生成的综述质量：  
[抗磷脂酶A2受体抗体相关膜性肾病与肿瘤关系的研究进展（gemini一气呵成版）.pdf](/uploads/short-url/gCPNafJg1vEgrOzYy8QgeN1fC1n.pdf)

  
[deepseek分块版.pdf](/uploads/short-url/A1bduBRTL0TLbyADzoPLhqxBUdd.pdf)

省流版比较结果：  

[![](https://linux.do/uploads/default/optimized/4X/a/c/a/aca1cbd15c7c2a6d814b2b3fc5d547c6d20b98a2_2_385x500.jpeg)

这两篇综述都是关于膜性肾病（MN）与肿瘤关联，特别是围绕抗磷1920×2493 353 KB

](https://linux.do/uploads/default/original/4X/a/c/a/aca1cbd15c7c2a6d814b2b3fc5d547c6d20b98a2.jpeg "这两篇综述都是关于膜性肾病（MN）与肿瘤关联，特别是围绕抗磷")

————————  
我理了一下我的逻辑：

我都用了分块检索。

GM的分块检索是在原聊天窗口中依次进行的，检索过程中调用的文本块会增加上下文负担——比如在检索第6个综述框架片段时，前面已经有一堆上文了（大概60k）

但是DS的分块检索是利用MCP在独立窗口进行的，只将撰写的综述片段成品返回给主窗口。检索过程中调用的文本块不会给主窗口。所以在检索综述框架片段，没有额外的上文。

然后我发现GM在有较多的无效上文负载的情况下，写出的效果仍然比DS好

  

![](https://linux.do/images/emoji/twemoji/heart.png?v=14)

heart

![](https://linux.do/images/emoji/twemoji/+1.png?v=14)

+1

7

​ ​ 回复

715 浏览量 10 赞 19 用户

 [![](https://linux.do/user_avatar/linux.do/user1164/96/426070_2.png)
 16](/u/user1164 "user1164") 

 [![](https://linux.do/user_avatar/linux.do/wxyttty/96/5813_2.png)
 6](/u/wxyttty "wxyttty") 

 [![](https://linux.do/letter_avatar/jcc/96/5_d44a9b381edc88181525e3c8350177ca.png)
 3](/u/jcc "jcc") 

 [![](https://linux.do/user_avatar/linux.do/1263403710/96/435746_2.png)
 2](/u/1263403710 "1263403710")

 [![](https://linux.do/user_avatar/linux.do/yuyuyang/96/307311_2.png)](/u/yuyuyang "yuyuyang") 

[![](https://linux.do/user_avatar/linux.do/homeworkkun/96/539992_2.png)
](/u/homeworkkun)

[作业君](/u/homeworkkun)

[homeworkkun](/u/homeworkkun)活跃用户 ![](https://linux.do/images/emoji/twemoji/tropical_fish.png?v=14) 

[3 小时](/t/topic/689522/2?u=niyan2025 "发布日期")

我理解的话，这个应该更体现的是调用工具的能力不足吧  
这个deepseek官方更新的时候也说了，目前只是支持了工具调用，能力还比较一般

  

1 个回复

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/wxyttty/96/5813_2.png)
](/u/wxyttty)

[Wxyttty](/u/wxyttty)

[3 小时](/t/topic/689522/3?u=niyan2025 "发布日期")

首先把分块和一气呵成比就已经不对了，不说工具调用能力，你比也应该都是比分块吧，其他模型分块效果也没有对比

  

1 个回复

1

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/user1164/96/426070_2.png)
](/u/user1164)

[欣 郁](/u/user1164)

[user1164](/u/user1164)

 ![](https://linux.do/user_avatar/linux.do/homeworkkun/48/539992_2.png)
 作业君

[3 小时](/t/topic/689522/4?u=niyan2025 "发布日期")

问题是在我的agent MCP中（即单独让LLM检索综述框架片段、写综述片段的工具中），deepseek-R1调用工具成功了（否则它不会返还文本）。这就是deepseek-R1处理6-7k input的能力不如高上下文负载下的gemini2.5pro的体现。。。

  

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/user1164/96/426070_2.png)
](/u/user1164)

[欣 郁](/u/user1164)

[user1164](/u/user1164)

 ![](https://linux.do/user_avatar/linux.do/wxyttty/48/5813_2.png)
 Wxyttty

[3 小时](/t/topic/689522/5?u=niyan2025 "发布日期")

![](https://linux.do/user_avatar/linux.do/wxyttty/48/5813_2.png)
 Wxyttty:

> 一气呵成

比较确实应该控制变量。

但理论上，一气呵成对LLM的要求应该更高才是……但gemini在更重的负载下表现还比ds-R1.1好。。

  

1 个回复

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/yuyuyang/96/307311_2.png)
](/u/yuyuyang)

[羽于羊](/u/yuyuyang)

[yuyuyang](/u/yuyuyang)一元复始 ![](https://linux.do/images/emoji/twemoji/man_technologist.png?v=14) 

[3 小时](/t/topic/689522/6?u=niyan2025 "发布日期")

deepseek上下文似乎最多只有128k吧

  

1 个回复

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/wxyttty/96/5813_2.png)
](/u/wxyttty)

[Wxyttty](/u/wxyttty)

 ![](https://linux.do/user_avatar/linux.do/user1164/48/426070_2.png)
 欣 郁

[3 小时](/t/topic/689522/7?u=niyan2025 "发布日期")

你要知道为什么llm要做长上下文，明明更难，就是在目前的拆分技术下1m就是比拆成10_100效果好，只是因为长上下文召回的原因，太长了效果保持不了，gm这种高召回率的1m打10_100有明显的优势，否则酒馆干脆分块用低上下文就好了

  

1 个回复

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/1263403710/96/435746_2.png)
](/u/1263403710)

[fromxiaobai](/u/1263403710)

[1263403710](/u/1263403710)文化宣导员

[3 小时](/t/topic/689522/8?u=niyan2025 "发布日期")

2.5pro的上下文可是1M，deepseek才多少，这个长文本基本是2.5pro一家独大吧，其他家没这么恐怖的算力

  

1 个回复

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/1341723/96/618032_2.png)
](/u/1341723)

[翠花上酸菜](/u/1341723)

[1341723](/u/1341723)

[3 小时](/t/topic/689522/9?u=niyan2025 "发布日期")

谷歌的上下文很长，能扔小说进去那种程度

  

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/luuby/96/318645_2.png)
](/u/luuby)

[糖々々](/u/luuby)

[luuby](/u/luuby)一元复始

 ![](https://linux.do/user_avatar/linux.do/yuyuyang/48/307311_2.png)
 羽于羊

[3 小时](/t/topic/689522/10?u=niyan2025 "发布日期")

128k都不到，32k就飞了

  

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/zhuanjiao/96/587325_2.png)
](/u/zhuanjiao)

[zhuanjiao](/u/zhuanjiao)

[3 小时](/t/topic/689522/11?u=niyan2025 "发布日期")

1m的上下文太爽了。不知道年内3.0能不能突破一下

  

​ ​ 回复

[![](https://linux.do/letter_avatar/jcc/96/5_d44a9b381edc88181525e3c8350177ca.png)
](/u/jcc)

[jcc](/u/jcc)

[3 小时](/t/topic/689522/12?u=niyan2025 "发布日期")

r1支持128k，可以试试和gemini一样，一次怼进去

如果不行的话，那就是不行了

通常情况，利用模型的原生的长上下文，都会比拆散了的多次调用更强

多次调用的方式是最后没办法的时候才考虑的，上下文够的时候先尝试使用上下文

  

1 个回复

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/user1164/96/426070_2.png)
](/u/user1164)

[欣 郁](/u/user1164)

[user1164](/u/user1164)

 ![](https://linux.do/user_avatar/linux.do/1263403710/48/435746_2.png)
 fromxiaobai

[3 小时](/t/topic/689522/13?u=niyan2025 "发布日期")

但在我的流程中，撰写某个综述片段时：

DS：0上文情况下，给6k input，要求2k-3k output  
GM：累积若干上文（最多大概60k）情况下，给6k input，要求2k-3k output

然后GM的表现比DS好（比如可以比较靠后的综述片段，也是GM比DS好）

  

1 个回复

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/dfobainx/96/306867_2.png)
](/u/DFobainx)

[DFobainx](/u/DFobainx)

Τακτικός

[3 小时](/t/topic/689522/14?u=niyan2025 "发布日期")

没有控制变量 它也不叫1.1  
![](https://linux.do/images/emoji/twemoji/joy.png?v=14)
 搞学术也得严谨一点

我的经验 告诉我  
好的 MCP 服务 其描述真的需要多轮的修正  
目标是在不同的模型都能得到相对一致表现

模型能力应该不是唯一的因素  
（例如还有上下文和最大输出  
当然 经验之谈 我不一定对的

  

​ ​ 回复

[![](https://linux.do/letter_avatar/a3members/96/5_d44a9b381edc88181525e3c8350177ca.png)
](/u/a3members)

[a3members](/u/a3members)

[3 小时](/t/topic/689522/15?u=niyan2025 "发布日期")

deepseek上下文 会不会因为显卡算力不够

  

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/user1164/96/426070_2.png)
](/u/user1164)

[欣 郁](/u/user1164)

[user1164](/u/user1164)

 ![](https://linux.do/letter_avatar/jcc/48/5_d44a9b381edc88181525e3c8350177ca.png)
 jcc

[3 小时](/t/topic/689522/16?u=niyan2025 "发布日期")

ds输出只有8k，所以我只能分块后再拼接。

但即使我让ds和gm的输出都限定8k，然后所有都一股脑输入，gm的质量也更高、截断率也更低

  

1 个回复

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/1263403710/96/435746_2.png)
](/u/1263403710)

[fromxiaobai](/u/1263403710)

[1263403710](/u/1263403710)文化宣导员

 ![](https://linux.do/user_avatar/linux.do/user1164/48/426070_2.png)
 欣 郁

[3 小时](/t/topic/689522/17?u=niyan2025 "发布日期")

deepseekr1.1明显是加强了思考逻辑，应该有多轮思考的逻辑，你6kinput 思考的token得有多少，还有输出的token，大概率已经是过了模型性能的甜点期了

  

2 个回复

​ ​ 回复

[![](https://linux.do/letter_avatar/jcc/96/5_d44a9b381edc88181525e3c8350177ca.png)
](/u/jcc)

[jcc](/u/jcc)

 ![](https://linux.do/user_avatar/linux.do/user1164/48/426070_2.png)
 欣 郁

[3 小时](/t/topic/689522/18?u=niyan2025 "发布日期")

r1支持128k输出的，至于效果怎样没试过，可以去试试

  

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/user1164/96/426070_2.png)
](/u/user1164)

[欣 郁](/u/user1164)

[user1164](/u/user1164)

 ![](https://linux.do/user_avatar/linux.do/wxyttty/48/5813_2.png)
 Wxyttty

[3 小时](/t/topic/689522/19?u=niyan2025 "发布日期")

![](https://linux.do/user_avatar/linux.do/wxyttty/48/5813_2.png)
 Wxyttty:

> 10_100_

但其实我的综述，每个部分都是相对独立的。你可看下我的结果，基本上每个部分，GM都比DS强 ![](https://linux.do/images/emoji/twemoji/rofl.png?v=14)

  

2 个回复

​ ​ 回复

[![](https://linux.do/user_avatar/linux.do/wxyttty/96/5813_2.png)
](/u/wxyttty)

[Wxyttty](/u/wxyttty)

 ![](https://linux.do/user_avatar/linux.do/user1164/48/426070_2.png)
 欣 郁

[3 小时](/t/topic/689522/20?u=niyan2025 "发布日期")

新版 R1 API 中 max\_tokens 参数的含义做了调整：现在 max\_tokens用于限制模型单次输出的总长度（包括思考过程），默认为 32K，最大为 64K。请 API 用户及时调整 max\_tokens 参数以防输出被提前截断。

  

​ ​ 回复

Invalid date Invalid date

Settings
========

🏷️ UTags - Add usertags to links

Other Extensions
----------------

[🔗 Links Helper](https://greasyfork.org/en/scripts/464541-links-helper)

[V2EX.REP - 专注提升 V2EX 主题回复浏览体验](https://greasyfork.org/en/scripts/466589-v2ex-rep-%E4%B8%93%E6%B3%A8%E6%8F%90%E5%8D%87-v2ex-%E4%B8%BB%E9%A2%98%E5%9B%9E%E5%A4%8D%E6%B5%8F%E8%A7%88%E4%BD%93%E9%AA%8C)

[v2ex.min - V2EX Minimalist (极简风格)](https://greasyfork.org/en/scripts/463552-v2ex-min-v2ex-%E6%9E%81%E7%AE%80%E9%A3%8E%E6%A0%BC)

[Replace Ugly Avatars](https://greasyfork.org/en/scripts/472616-replace-ugly-avatars)

↑↓⇔⇧⇩